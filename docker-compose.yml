services:
  triton:
    image: nvcr.io/nvidia/tritonserver:24.09-py3 # 版本可改
    container_name: triton
    restart: unless-stopped
    shm_size: "1g"
    ulimits:
      memlock: -1
      stack: 67108864
    ports:
      - "8000:8000" # HTTP
      - "8001:8001" # gRPC
      - "8002:8002" # Metrics
    environment:
      # 視需求開啟：
      TRITON_LOG_VERBOSE: "1"
      TRITON_MODEL_CONTROL_MODE: "poll" # 開啟熱更新（建議）
      # CUDA_VISIBLE_DEVICES: "0"           # 指定要用哪張 GPU
    volumes:
      - ./models:/models
    command: >
      tritonserver
      --model-repository=/models
      --strict-model-config=false
      --exit-on-error=false
      --allow-http=true
      --allow-grpc=true
      --metrics-port=8002
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"] # 讓容器吃到 GPU
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 12
